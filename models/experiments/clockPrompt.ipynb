{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323480c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0265be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshgupta/Documents/mirage-ai-techjam/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hiteshgupta/Documents/mirage-ai-techjam/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cat_img.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     24\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     25\u001b[0m ])\n\u001b[1;32m     27\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_img.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m x\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/mirage-ai-techjam/venv/lib/python3.10/site-packages/PIL/Image.py:3493\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m   3492\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m-> 3493\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3494\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cat_img.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "imagenet_classes_file = \"imagenet_classes.txt\"\n",
    "download_url(url, \".\", imagenet_classes_file, None)\n",
    "with open(imagenet_classes_file) as f:\n",
    "    idx_to_class = [line.strip() for line in f.readlines()]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "resnet = models.resnet50(pretrained=True).eval().to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "image_path = \"cat_img.png\"\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "x = transform(img).unsqueeze(0).to(device)\n",
    "x.requires_grad = True\n",
    "\n",
    "class_prompt = \"tiger cat\"  \n",
    "target_class_idx = idx_to_class.index(class_prompt)\n",
    "\n",
    "output = resnet(x)\n",
    "loss = output[0, target_class_idx] \n",
    "\n",
    "loss.backward()\n",
    "epsilon = 0.01\n",
    "perturbed = x - epsilon * x.grad.data.sign()\n",
    "perturbed = torch.clamp(perturbed, 0, 1).detach()\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "save_image(x, \"original.png\", normalize=True)\n",
    "save_image(perturbed, \"cloaked.png\", normalize=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs_original = F.softmax(resnet(x), dim=1)[0]\n",
    "    probs_cloaked = F.softmax(resnet(perturbed), dim=1)[0]\n",
    "\n",
    "original_top = torch.topk(probs_original, 5)\n",
    "cloaked_top = torch.topk(probs_cloaked, 5)\n",
    "\n",
    "print(\"ðŸ” Top classes before cloaking:\")\n",
    "for i in range(5):\n",
    "    print(f\"{idx_to_class[original_top.indices[i]]}: {original_top.values[i].item():.4f}\")\n",
    "\n",
    "print(\"\\nðŸ‘» Top classes after cloaking:\")\n",
    "for i in range(5):\n",
    "    print(f\"{idx_to_class[cloaked_top.indices[i]]}: {cloaked_top.values[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d70695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class index: 299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m cand\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_adv\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 50\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m \u001b[43mone_pixel_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pixels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m save_image(x_adv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloaked_pixel.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m new_pred \u001b[38;5;241m=\u001b[39m model(x_adv)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mone_pixel_attack\u001b[0;34m(x, model, iters, n_pixels, epsilon)\u001b[0m\n\u001b[1;32m     38\u001b[0m     cand[\u001b[38;5;241m0\u001b[39m, :, r, c] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(cand[\u001b[38;5;241m0\u001b[39m, :, r, c] \u001b[38;5;241m+\u001b[39m delta, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m model(cand)\n\u001b[0;32m---> 41\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m!=\u001b[39m base_pred:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess at iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, predicted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = models.resnet50(pretrained=True).eval().to(device)\n",
    "\n",
    "# Load image\n",
    "img = Image.open(\"cat_img.png\").convert(\"RGB\")\n",
    "transform = transforms.Compose([ transforms.ToTensor()])\n",
    "x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Predict baseline\n",
    "with torch.no_grad():\n",
    "    base_pred = model(x).argmax(dim=1).item()\n",
    "\n",
    "print(f\"Base class index: {base_pred}\")\n",
    "\n",
    "# --- One Pixel Attack ---\n",
    "def one_pixel_attack(x, model, iters=3000, n_pixels=1, epsilon=0.03):\n",
    "    _, _, H, W = x.shape\n",
    "    x_adv = x.clone()\n",
    "\n",
    "    for i in range(iters):\n",
    "        cand = x.clone()\n",
    "\n",
    "        for _ in range(n_pixels):\n",
    "            r = random.randint(0, H-1)\n",
    "            c = random.randint(0, W-1)\n",
    "\n",
    "            # small local perturbation, not random RGB\n",
    "            delta = torch.randn(3).to(x.device) * epsilon\n",
    "            cand[0, :, r, c] = torch.clamp(cand[0, :, r, c] + delta, 0, 1)\n",
    "\n",
    "        out = model(cand)\n",
    "        pred = out.argmax(dim=1).item()\n",
    "\n",
    "        if pred != base_pred:\n",
    "            print(f\"Success at iter {i}, predicted {pred}\")\n",
    "            return cand.detach()\n",
    "\n",
    "    return x_adv.detach()\n",
    "\n",
    "\n",
    "x_adv = one_pixel_attack(x, model, iters=1000, n_pixels=10)\n",
    "save_image(x_adv, \"cloaked_pixel.png\", normalize=False)\n",
    "\n",
    "new_pred = model(x_adv).argmax(dim=1).item()\n",
    "print(f\"New class index after one-pixel attack: {new_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = models.resnet50(pretrained=True).eval().to(device)\n",
    "\n",
    "# Load full resolution image\n",
    "orig_img = Image.open(\"cat_img.png\").convert(\"RGB\")\n",
    "W_orig, H_orig = orig_img.size\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "x_small = transform(orig_img).unsqueeze(0).to(device)\n",
    "x_small.requires_grad = False  # not using FGSM/PGD now\n",
    "\n",
    "# Predict baseline\n",
    "with torch.no_grad():\n",
    "    base_pred = model(x_small).argmax(dim=1).item()\n",
    "print(\"Base class:\", base_pred)\n",
    "\n",
    "# Convert original to tensor\n",
    "orig_tensor = transforms.ToTensor()(orig_img).to(device)\n",
    "\n",
    "# ========== One-Pixel Attack (small version) ==========\n",
    "def one_pixel_attack_highres(x_small, orig_tensor, model, iters=3000, n_pixels=3, epsilon=0.03):\n",
    "\n",
    "    _, _, H_small, W_small = x_small.shape\n",
    "    _, _, H_big, W_big = orig_tensor.unsqueeze(0).shape\n",
    "\n",
    "    x_adv_small = x_small.clone()\n",
    "    orig_big = orig_tensor.clone()\n",
    "\n",
    "    for i in range(iters):\n",
    "        cand_small = x_small.clone()\n",
    "        cand_big = orig_big.clone()\n",
    "\n",
    "        for _ in range(n_pixels):\n",
    "            # choose attack pixel in low-res\n",
    "            r_small = random.randint(0, H_small - 1)\n",
    "            c_small = random.randint(0, W_small - 1)\n",
    "\n",
    "            # small delta\n",
    "            delta = torch.randn(3).to(device) * epsilon\n",
    "            cand_small[0, :, r_small, c_small] = torch.clamp(\n",
    "                cand_small[0, :, r_small, c_small] + delta,\n",
    "                0, 1\n",
    "            )\n",
    "\n",
    "            # === Map to high-res ===\n",
    "            r_big = int(r_small * H_big / H_small)\n",
    "            c_big = int(c_small * W_big / W_small)\n",
    "\n",
    "            cand_big[:, r_big, c_big] = torch.clamp(\n",
    "                cand_big[:, r_big, c_big] + delta,\n",
    "                0, 1\n",
    "            )\n",
    "\n",
    "        out = model(cand_small)\n",
    "        pred = out.argmax(dim=1).item()\n",
    "\n",
    "        if pred != base_pred:\n",
    "            print(f\"Success at iter {i}, predicted {pred}\")\n",
    "            return cand_big  # RETURN HIGH-RES CLOAKED VERSION\n",
    "\n",
    "    return orig_big\n",
    "\n",
    "\n",
    "x_adv_big = one_pixel_attack_highres(x_small, orig_tensor, model, iters=2000, n_pixels=5)\n",
    "\n",
    "# Save high-resolution result\n",
    "save_image(x_adv_big, \"cloaked_highres.png\", normalize=False)\n",
    "print(\"Saved cloaked_highres.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ddf96c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Top classes before cloaking:\n",
      "tiger cat: 0.6232\n",
      "Egyptian cat: 0.2579\n",
      "tabby: 0.0970\n",
      "radiator: 0.0034\n",
      "lynx: 0.0029\n",
      "\n",
      "ðŸ‘» Top classes after cloaking:\n",
      "tiger cat: 0.2612\n",
      "Egyptian cat: 0.2580\n",
      "tabby: 0.0952\n",
      "paper towel: 0.0179\n",
      "lynx: 0.0174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.utils import save_image\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Download imagenet labels\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "imagenet_classes_file = \"imagenet_classes.txt\"\n",
    "download_url(url, \".\", imagenet_classes_file, None)\n",
    "with open(imagenet_classes_file) as f:\n",
    "    idx_to_class = [line.strip() for line in f.readlines()]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "resnet = models.resnet50(pretrained=True).eval().to(device)\n",
    "\n",
    "# Load ORIGINAL high-res image\n",
    "image_path = \"cat_img.png\"\n",
    "orig_img = Image.open(image_path).convert(\"RGB\")\n",
    "orig_w, orig_h = orig_img.size\n",
    "\n",
    "# Preprocess for model (224x224)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "x_small = preprocess(orig_img).unsqueeze(0).to(device)\n",
    "x_small.requires_grad = True\n",
    "\n",
    "# ---- Choose the target class ----\n",
    "class_prompt = \"tiger cat\"\n",
    "target_class_idx = idx_to_class.index(class_prompt)\n",
    "\n",
    "# ---- Forward ----\n",
    "output = resnet(x_small)\n",
    "loss = output[0, target_class_idx]\n",
    "\n",
    "# ---- Backward ----\n",
    "loss.backward()\n",
    "\n",
    "epsilon = 0.01\n",
    "grad_sign = x_small.grad.data.sign()\n",
    "\n",
    "# ---- Generate 224 Ã— 224 perturbed image ----\n",
    "pert_small = torch.clamp(x_small - epsilon * grad_sign, 0, 1).detach()\n",
    "\n",
    "# ---- Map the perturbation back to original resolution ----\n",
    "# Resize perturbed-small to original shape using bilinear upsampling\n",
    "pert_big = torch.nn.functional.interpolate(\n",
    "    pert_small, size=(orig_h, orig_w), mode='bilinear', align_corners=False\n",
    ")\n",
    "\n",
    "# But interpolation scales pixel values - we want ONLY the perturbation.\n",
    "# So compute Î´ = pert_small - x_small, then upscale Î´ and add to original.\n",
    "\n",
    "delta_small = pert_small - x_small\n",
    "\n",
    "delta_big = torch.nn.functional.interpolate(\n",
    "    delta_small, size=(orig_h, orig_w), mode='bilinear', align_corners=False\n",
    ")\n",
    "\n",
    "# Convert original high-res to tensor\n",
    "orig_tensor = transforms.ToTensor()(orig_img).unsqueeze(0).to(device)\n",
    "\n",
    "perturbed_highres = torch.clamp(orig_tensor + delta_big, 0, 1)\n",
    "\n",
    "# ---- Save images ----\n",
    "save_image(orig_tensor, \"original_fixed.png\", normalize=False)\n",
    "save_image(perturbed_highres, \"cloaked_fixed.png\", normalize=False)\n",
    "\n",
    "# ---- Check predictions ----\n",
    "with torch.no_grad():\n",
    "    # prediction on original\n",
    "    x_for_pred_orig = preprocess(orig_img).unsqueeze(0).to(device)\n",
    "    probs_original = F.softmax(resnet(x_for_pred_orig), dim=1)[0]\n",
    "\n",
    "    # prediction on perturbed\n",
    "    # resize perturbed highres before model\n",
    "    pert_resized = preprocess(\n",
    "        transforms.ToPILImage()(perturbed_highres.squeeze(0).cpu())\n",
    "    ).unsqueeze(0).to(device)\n",
    "    probs_cloaked = F.softmax(resnet(pert_resized), dim=1)[0]\n",
    "\n",
    "orig_top = torch.topk(probs_original, 5)\n",
    "cloaked_top = torch.topk(probs_cloaked, 5)\n",
    "\n",
    "print(\"ðŸ” Top classes before cloaking:\")\n",
    "for i in range(5):\n",
    "    print(f\"{idx_to_class[orig_top.indices[i]]}: {orig_top.values[i].item():.4f}\")\n",
    "\n",
    "print(\"\\nðŸ‘» Top classes after cloaking:\")\n",
    "for i in range(5):\n",
    "    print(f\"{idx_to_class[cloaked_top.indices[i]]}: {cloaked_top.values[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bf0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepface\n",
      "  Downloading deepface-0.0.95-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128 kB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fire>=0.4.0\n",
      "  Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn>=20.1.0\n",
      "  Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (2.2.4)\n",
      "Collecting tensorflow>=1.9.0\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-macosx_12_0_arm64.whl (200.4 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200.4 MB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Flask>=1.1.2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (3.1.0)\n",
      "Collecting retina-face>=0.0.14\n",
      "  Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
      "Collecting mtcnn>=0.1.0\n",
      "  Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.9 MB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gdown>=3.10.1\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (11.2.1)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.27.1 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (2.32.3)\n",
      "Collecting opencv-python>=4.5.5.64\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "Collecting pandas>=0.23.4\n",
      "  Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.8 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras>=2.2.0\n",
      "  Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5 MB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask-cors>=4.0.1 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from deepface) (5.0.1)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: blinker>=1.9 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 106 kB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from gdown>=3.10.1->deepface) (3.18.0)\n",
      "Requirement already satisfied: packaging in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.15.1-cp310-cp310-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting optree\n",
      "  Downloading optree-0.17.0-cp310-cp310-macosx_11_0_arm64.whl (337 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 337 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ml-dtypes\n",
      "  Downloading ml_dtypes-0.5.3-cp310-cp310-macosx_10_9_universal2.whl (667 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 667 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lz4>=4.3.3\n",
      "  Downloading lz4-4.4.5-cp310-cp310-macosx_11_0_arm64.whl (207 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.4.2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->deepface) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from requests>=2.27.1->deepface) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from requests>=2.27.1->deepface) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from requests>=2.27.1->deepface) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-2.0.1-cp310-cp310-macosx_11_0_arm64.whl (61 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google_pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-macosx_11_0_universal2.whl (11.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.8 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.8 MB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface) (57.4.0)\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71 kB 3.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=5.28.0\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427 kB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.20.0\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.5 MB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72 kB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107 kB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages (from rich->keras>=2.2.0->deepface) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, markdown-it-py, wheel, tensorboard-data-server, soupsieve, rich, PySocks, protobuf, optree, namex, ml-dtypes, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, beautifulsoup4, astunparse, tzdata, tensorflow, pytz, opencv-python, lz4, gdown, retina-face, pandas, mtcnn, gunicorn, fire, deepface\n",
      "Successfully installed PySocks-1.7.1 absl-py-2.3.1 astunparse-1.6.3 beautifulsoup4-4.14.2 deepface-0.0.95 fire-0.7.1 flatbuffers-25.9.23 gast-0.6.0 gdown-5.2.0 google-pasta-0.2.0 grpcio-1.76.0 gunicorn-23.0.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 lz4-4.4.5 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.3 mtcnn-1.0.0 namex-0.1.0 opencv-python-4.12.0.88 opt-einsum-3.4.0 optree-0.17.0 pandas-2.3.3 protobuf-6.33.1 pytz-2025.2 retina-face-0.0.17 rich-14.2.0 soupsieve-2.8 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 tzdata-2025.2 wheel-0.45.1 wrapt-2.0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/hiteshgupta/Documents/mirage-ai/ven1/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eabe13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/hiteshgupta/Documents/mirage-ai/ven1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import models\n",
    "\n",
    "class VGGFace:\n",
    "    @staticmethod\n",
    "    def loadModel():\n",
    "        # Lightweight fallback: use ImageNet-pretrained VGG16 and expose 4096-dim embeddings\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        # drop final classification layer so forward() returns the penultimate 4096-d feature\n",
    "        vgg.classifier = torch.nn.Sequential(*list(vgg.classifier.children())[:-1])\n",
    "        return vgg\n",
    "    \n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# MODELS\n",
    "# -------------------------\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "model = VGGFace.loadModel()\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n",
    "# TRANSFORMS\n",
    "# -------------------------\n",
    "face_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# FAWKES-STYLE FACE CLOAKING (UNTARGETED + TARGETED)\n",
    "# ==========================================================\n",
    "def cloak_face_fawkes(\n",
    "    orig_img, \n",
    "    intensity=0.01,\n",
    "    method=\"fgsm\",\n",
    "    targeted=False,\n",
    "    target_identity_img=None\n",
    "):\n",
    "    \"\"\"\n",
    "    orig_img = PIL Image\n",
    "    intensity = epsilon\n",
    "    targeted = True/False\n",
    "    target_identity_img = PIL Image (for targeted attack)\n",
    "    \"\"\"\n",
    "\n",
    "    orig_w, orig_h = orig_img.size\n",
    "\n",
    "    # ---- 1. Detect face ----\n",
    "    boxes, probs = mtcnn.detect(orig_img)\n",
    "    if boxes is None:\n",
    "        print(\"No face detected\")\n",
    "        return orig_img\n",
    "\n",
    "    x1, y1, x2, y2 = map(int, boxes[0])\n",
    "\n",
    "    face_crop = orig_img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # ---- 2. Preprocess for VGG-Face ----\n",
    "    face_tensor = face_preprocess(face_crop).unsqueeze(0).to(device)\n",
    "    face_tensor.requires_grad = True\n",
    "\n",
    "    # ---- 3. Compute original embedding ----\n",
    "    orig_emb = model(face_tensor).detach()\n",
    "\n",
    "    # ---- 4. If targeted, compute target embedding ----\n",
    "    if targeted:\n",
    "        if target_identity_img is None:\n",
    "            raise ValueError(\"Targeted cloaking requires target_identity_img\")\n",
    "\n",
    "        target_face_tensor = face_preprocess(target_identity_img).unsqueeze(0).to(device)\n",
    "        target_emb = model(target_face_tensor).detach()\n",
    "    else:\n",
    "        target_emb = None\n",
    "\n",
    "    # ---- 5. Define losses ----\n",
    "    def untargeted_loss(adv_emb, orig_emb):\n",
    "        # maximize distance\n",
    "        return -torch.nn.functional.cosine_similarity(adv_emb, orig_emb).mean()\n",
    "\n",
    "    def targeted_loss(adv_emb, target_emb):\n",
    "        # minimize distance to target identity\n",
    "        return torch.nn.functional.cosine_similarity(adv_emb, target_emb).mean()\n",
    "\n",
    "    # ---- 6. FGSM ----\n",
    "    if method == \"fgsm\":\n",
    "        emb = model(face_tensor)\n",
    "\n",
    "        if targeted:\n",
    "            loss = targeted_loss(emb, target_emb)\n",
    "        else:\n",
    "            loss = untargeted_loss(emb, orig_emb)\n",
    "\n",
    "        loss.backward()\n",
    "        epsilon = intensity\n",
    "\n",
    "        adv_small = torch.clamp(\n",
    "            face_tensor + epsilon * face_tensor.grad.sign(),\n",
    "            0, 1\n",
    "        ).detach()\n",
    "\n",
    "    # ---- 7. PGD ----\n",
    "    elif method == \"pgd\":\n",
    "        adv_small = face_tensor.clone()\n",
    "        epsilon = intensity\n",
    "        alpha = epsilon / 3\n",
    "        steps = 5\n",
    "\n",
    "        for _ in range(steps):\n",
    "            adv_small.requires_grad = True\n",
    "            emb = model(adv_small)\n",
    "\n",
    "            if targeted:\n",
    "                loss = targeted_loss(emb, target_emb)\n",
    "            else:\n",
    "                loss = untargeted_loss(emb, orig_emb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            adv_small = adv_small + alpha * adv_small.grad.sign()\n",
    "            adv_small = torch.min(torch.max(adv_small, face_tensor - epsilon), face_tensor + epsilon)\n",
    "            adv_small = torch.clamp(adv_small, 0, 1).detach()\n",
    "\n",
    "    # ---- 8. Get perturbation Î´ ----\n",
    "    delta_small = adv_small - face_tensor\n",
    "\n",
    "    # ---- 9. Upscale Î´ to original resolution ----\n",
    "    face_H = y2 - y1\n",
    "    face_W = x2 - x1\n",
    "\n",
    "    delta_big = torch.nn.functional.interpolate(\n",
    "        delta_small,\n",
    "        size=(face_H, face_W),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    # ---- 10. Apply only to face region ----\n",
    "    orig_tensor = to_tensor(orig_img).unsqueeze(0).to(device)\n",
    "    perturbed_tensor = orig_tensor.clone()\n",
    "\n",
    "    perturbed_tensor[:, :, y1:y2, x1:x2] = torch.clamp(\n",
    "        perturbed_tensor[:, :, y1:y2, x1:x2] + delta_big,\n",
    "        0, 1\n",
    "    )\n",
    "\n",
    "    return perturbed_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81cf0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"input/face.png\").convert(\"RGB\")\n",
    "adv = cloak_face_fawkes(img, intensity=0.01, targeted=False)\n",
    "save_image(adv, \"cloaked_output/face_cloaked_untargeted.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270c470",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'elif' statement on line 84 (384942579.py, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 85\u001b[0;36m\u001b[0m\n\u001b[0;31m    adv_small = face_small.clone()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'elif' statement on line 84\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# MODELS\n",
    "# -------------------------\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "\n",
    "facenet = InceptionResnetV1(pretrained=\"vggface2\").eval().to(device)\n",
    "\n",
    "# -------------------------\n",
    "# TRANSFORMS\n",
    "# -------------------------\n",
    "face_preprocess = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),   # FaceNet input size\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# FACE CLOAKING WITH FACENET (TARGETED + UNTARGETED)\n",
    "# RETURNS METRICS\n",
    "# ==========================================================\n",
    "def cloak_face_facenet(\n",
    "    orig_img,\n",
    "    intensity=0.01,\n",
    "    method=\"fgsm\",\n",
    "    targeted=False,\n",
    "    target_identity_img=None\n",
    "):\n",
    "    orig_w, orig_h = orig_img.size\n",
    "\n",
    "    # ---- 1. Detect face ----\n",
    "    boxes, probs = mtcnn.detect(orig_img)\n",
    "    if boxes is None:\n",
    "        print(\"No face detected\")\n",
    "        return orig_img, {}\n",
    "\n",
    "    x1, y1, x2, y2 = map(int, boxes[0])\n",
    "\n",
    "    face_crop = orig_img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    # ---- 2. Preprocess ----\n",
    "    face_small = face_preprocess(face_crop).unsqueeze(0).to(device)\n",
    "    face_small.requires_grad = True\n",
    "\n",
    "    # ---- 3. Original face embedding ----\n",
    "    orig_emb = facenet(face_small).detach()\n",
    "\n",
    "    # ---- 4. Optional targeted embedding ----\n",
    "    if targeted:\n",
    "        if target_identity_img is None:\n",
    "            raise ValueError(\"Targeted attack requires target identity image\")\n",
    "        target_face = face_preprocess(target_identity_img).unsqueeze(0).to(device)\n",
    "        target_emb = facenet(target_face).detach()\n",
    "    else:\n",
    "        target_emb = None\n",
    "\n",
    "    # ---- 5. Loss functions ----\n",
    "    def untargeted_loss(adv_emb, orig_emb):\n",
    "        return -F.cosine_similarity(adv_emb, orig_emb).mean()\n",
    "\n",
    "    def targeted_loss(adv_emb, target_emb):\n",
    "        return F.cosine_similarity(adv_emb, target_emb).mean()\n",
    "\n",
    "    # ---- 6. FGSM Attack ----\n",
    "    if method == \"fgsm\":\n",
    "        emb = facenet(face_small)\n",
    "        loss = targeted_loss(emb, target_emb) if targeted else untargeted_loss(emb, orig_emb)\n",
    "        loss.backward()\n",
    "\n",
    "        epsilon = intensity\n",
    "        adv_small = torch.clamp(face_small + epsilon * face_small.grad.sign(), 0, 1).detach()\n",
    "\n",
    "    # ---- 7. PGD Attack ----\n",
    "    elif method == \"pgd\":\n",
    "        adv_small = face_small.clone()\n",
    "        epsilon = intensity\n",
    "        alpha = epsilon / 3\n",
    "        steps = 7\n",
    "\n",
    "        for _ in range(steps):\n",
    "            adv_small = adv_small.detach()\n",
    "            adv_small.requires_grad_(True)\n",
    "\n",
    "            emb = facenet(adv_small)\n",
    "\n",
    "            if targeted:\n",
    "                loss = targeted_loss(emb, target_emb)\n",
    "            else:\n",
    "                loss = untargeted_loss(emb, orig_emb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient step\n",
    "            adv_small = adv_small + alpha * adv_small.grad.sign()\n",
    "\n",
    "            # project back into epsilon-ball\n",
    "            adv_small = torch.min(torch.max(adv_small, face_small - epsilon), face_small + epsilon)\n",
    "\n",
    "            # keep valid pixel range\n",
    "            adv_small = torch.clamp(adv_small, 0, 1)\n",
    "\n",
    "\n",
    "    # ---- 8. Compute Î´ (perturbation) ----\n",
    "    delta_small = adv_small - face_small\n",
    "\n",
    "    # ---- 9. Upscale Î´ to original face resolution ----\n",
    "    face_H = y2 - y1\n",
    "    face_W = x2 - x1\n",
    "\n",
    "    delta_big = torch.nn.functional.interpolate(\n",
    "        delta_small,\n",
    "        size=(face_H, face_W),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    # ---- 10. Apply only to the face region ----\n",
    "    orig_tensor = to_tensor(orig_img).unsqueeze(0).to(device)\n",
    "    perturbed_tensor = orig_tensor.clone()\n",
    "\n",
    "    perturbed_tensor[:, :, y1:y2, x1:x2] = torch.clamp(\n",
    "        orig_tensor[:, :, y1:y2, x1:x2] + delta_big,\n",
    "        0, 1\n",
    "    )\n",
    "\n",
    "    # ---- 11. Compute metrics ----\n",
    "    adv_face_crop = transforms.ToPILImage()(perturbed_tensor[0, :, y1:y2, x1:x2].cpu())\n",
    "    adv_face_small = face_preprocess(adv_face_crop).unsqueeze(0).to(device)\n",
    "    adv_emb = facenet(adv_face_small).detach()\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    # Untargeted metrics\n",
    "    orig_sim = F.cosine_similarity(orig_emb, orig_emb).item()          # =1\n",
    "    adv_sim = F.cosine_similarity(orig_emb, adv_emb).item()\n",
    "    emb_dist = (orig_emb - adv_emb).norm().item()\n",
    "\n",
    "    metrics[\"cosine_similarity_before\"] = 1.0\n",
    "    metrics[\"cosine_similarity_after\"] = adv_sim\n",
    "    metrics[\"embedding_distance_original_vs_adv\"] = emb_dist\n",
    "    metrics[\"similarity_drop\"] = 1.0 - adv_sim\n",
    "\n",
    "    # Targeted metrics\n",
    "    if targeted:\n",
    "        tgt_sim_before = F.cosine_similarity(orig_emb, target_emb).item()\n",
    "        tgt_sim_after = F.cosine_similarity(adv_emb, target_emb).item()\n",
    "\n",
    "        metrics[\"target_similarity_before\"] = tgt_sim_before\n",
    "        metrics[\"target_similarity_after\"] = tgt_sim_after\n",
    "        metrics[\"push_toward_target\"] = tgt_sim_after - tgt_sim_before\n",
    "\n",
    "    return perturbed_tensor, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fb94fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine_similarity_before': 1.0, 'cosine_similarity_after': 0.6769204139709473, 'embedding_distance_original_vs_adv': 0.8038402199745178, 'similarity_drop': 0.32307958602905273}\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"input/face.png\").convert(\"RGB\")\n",
    "adv, metrics = cloak_face_facenet(img, intensity=0.03, targeted=False)\n",
    "save_image(adv, \"cloaked_output/face_cloaked_untargeted.png\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61c7ddf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m my_face \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput/face.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m fake_identity \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput/celeb.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m adv, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcloak_face_facenet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmy_face\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.015\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargeted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_identity_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_identity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m save_image(adv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface_cloaked_targeted.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n",
      "Cell \u001b[0;32mIn[22], line 91\u001b[0m, in \u001b[0;36mcloak_face_facenet\u001b[0;34m(orig_img, intensity, method, targeted, target_identity_img)\u001b[0m\n\u001b[1;32m     88\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m---> 91\u001b[0m     \u001b[43madv_small\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     emb \u001b[38;5;241m=\u001b[39m facenet(adv_small)\n\u001b[1;32m     93\u001b[0m     loss \u001b[38;5;241m=\u001b[39m targeted_loss(emb, target_emb) \u001b[38;5;28;01mif\u001b[39;00m targeted \u001b[38;5;28;01melse\u001b[39;00m untargeted_loss(emb, orig_emb)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "my_face = Image.open(\"input/face.png\").convert(\"RGB\")\n",
    "fake_identity = Image.open(\"input/celeb.jpg\").convert(\"RGB\")\n",
    "\n",
    "adv, metrics = cloak_face_facenet(\n",
    "    my_face,\n",
    "    intensity=0.015,\n",
    "    targeted=True,\n",
    "    target_identity_img=fake_identity,\n",
    "    method=\"pgd\"\n",
    ")\n",
    "\n",
    "save_image(adv, \"face_cloaked_targeted.png\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907cc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
