{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3a248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import clip\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load models\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def generate_cloak(image_pil, epsilon=0.003):\n",
    "    # Resize and convert to tensor\n",
    "    transform_to_tensor = T.Compose([\n",
    "        T.Resize((224, 224)),      # Required for CLIP\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform_to_tensor(image_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Clone and normalize with grad enabled\n",
    "    image_clip = image_tensor.clone().detach().requires_grad_(True)\n",
    "    normalize_clip = T.Normalize(mean=(0.4815, 0.4578, 0.4082), std=(0.2686, 0.2613, 0.2758))\n",
    "    image_clip_norm = normalize_clip(image_clip)\n",
    "\n",
    "    # Text prompts\n",
    "    text_prompt = clip.tokenize([\"a painting of a cat\", \"an abstract landscape\"]).to(device)\n",
    "    text_features = clip_model.encode_text(text_prompt).mean(dim=0, keepdim=True)\n",
    "\n",
    "    # CLIP forward and loss\n",
    "    image_features = clip_model.encode_image(image_clip_norm)\n",
    "    clip_loss = -torch.cosine_similarity(image_features, text_features).mean()\n",
    "    clip_loss.backward()\n",
    "\n",
    "    # Cosine similarity of the original image and text (before perturbation)\n",
    "    original_image_features = clip_model.encode_image(image_clip_norm)\n",
    "    original_image_features = original_image_features / original_image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    original_cosine_similarity_score = torch.cosine_similarity(original_image_features, text_features).item()\n",
    "    print(f\"Original Cosine Similarity Score: {original_cosine_similarity_score}\")\n",
    "\n",
    "    # FGSM perturbation\n",
    "    perturbed = image_clip + epsilon * image_clip.grad.data.sign()\n",
    "    perturbed = torch.clamp(perturbed, 0, 1)\n",
    "\n",
    "    # Cosine similarity of the perturbed image and text\n",
    "    perturbed_image_features = clip_model.encode_image(normalize_clip(perturbed))\n",
    "    perturbed_image_features = perturbed_image_features / perturbed_image_features.norm(dim=-1, keepdim=True)\n",
    "    cosine_similarity_score = torch.cosine_similarity(perturbed_image_features, original_image_features).item()\n",
    "    print(f\"Perturbed Cosine Similarity Score: {cosine_similarity_score}\")\n",
    "    \n",
    "    embeddings = [perturbed_image_features, original_image_features]\n",
    "    embeddings = torch.cat([perturbed_image_features, original_image_features], dim=0)\n",
    "    reduced = TSNE(n_components=2).fit_transform(embeddings.detach().cpu().numpy())\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "    # Convert back to PIL\n",
    "    cloaked_img = T.ToPILImage()(perturbed.squeeze().detach().cpu())\n",
    "    return cloaked_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba8440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Cosine Similarity Score: 0.188808873295784\n",
      "Perturbed Cosine Similarity Score: 0.9323042631149292\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m input_art_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m      \u001b[38;5;66;03m# Your input image path\u001b[39;00m\n\u001b[1;32m     25\u001b[0m output_cloak_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloaked_output/art_cloaked.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Output path\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mcloak_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_art_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_cloak_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mcloak_and_save\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(input_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Cloak the image\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m cloaked_image \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_cloak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the cloaked image\u001b[39;00m\n\u001b[1;32m     17\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(output_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m, in \u001b[0;36mgenerate_cloak\u001b[0;34m(image_pil, epsilon)\u001b[0m\n\u001b[1;32m     58\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [perturbed_image_features, original_image_features]\n\u001b[1;32m     59\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([perturbed_image_features, original_image_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(reduced[:, \u001b[38;5;241m0\u001b[39m], reduced[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Convert back to PIL\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/mirage-ai/ven1/lib/python3.10/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/mirage-ai/ven1/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mirage-ai/ven1/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1177\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/Documents/mirage-ai/ven1/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:862\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Import the generate_cloak function from the module (assuming it's in cloak.py)\n",
    "# from cloak import generate_cloak\n",
    "\n",
    "# Paste the generate_cloak function code here if not using as a module.\n",
    "\n",
    "def cloak_and_save(input_path, output_path):\n",
    "    # Load input image\n",
    "    image = Image.open(input_path).convert(\"RGB\")\n",
    "    \n",
    "    # Cloak the image\n",
    "    cloaked_image = generate_cloak(image, 0.001)\n",
    "    \n",
    "    # Save the cloaked image\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    cloaked_image.save(output_path)\n",
    "\n",
    "    print(f\"Cloaked image saved to {output_path}\")\n",
    "\n",
    "# === Example ===\n",
    "if __name__ == \"__main__\":\n",
    "    input_art_path = \"image.png\"      # Your input image path\n",
    "    output_cloak_path = \"cloaked_output/art_cloaked.jpg\"  # Output path\n",
    "\n",
    "    cloak_and_save(input_art_path, output_cloak_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings: [original_embedding, cloaked_embedding, ...reference_embeddings]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven1 (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
